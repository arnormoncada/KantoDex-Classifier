{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokémon Image Classifier Notebook\n",
    "\n",
    "This notebook guides you through the process of downloading the Pokémon dataset, training a classification model, evaluating its performance, and performing inference on sample images. The key steps include:\n",
    "\n",
    "1. **Setup and Imports**\n",
    "2. **Downloading and Organizing the Dataset**\n",
    "3. **Data Loading and Preprocessing**\n",
    "4. **Model Initialization**\n",
    "5. **Training the Model**\n",
    "6. **Visualizing Training Metrics**\n",
    "7. **Model Evaluation and Confusion Matrix**\n",
    "8. **Inference on Sample Images**\n",
    "\n",
    "\n",
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all the necessary libraries and set up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from timm.data import Mixup  # New: Using Mixup from timm\n",
    "from torch import cuda, device, nn, optim\n",
    "from torch.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from download_dataset import main as download_dataset\n",
    "from src.data.data_loader import load_data\n",
    "from src.utils.helpers import save_checkpoint\n",
    "from src.utils.metrics import MetricsCalculator\n",
    "from src.visualization.tensorboard_logger import TensorBoardLogger\n",
    "from train import (\n",
    "    initialize_model,\n",
    "    initialize_optimizer,\n",
    "    initialize_scheduler,\n",
    "    load_config,\n",
    "    train_epoch,\n",
    ")\n",
    "\n",
    "# Ensure plots are displayed inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Check for GPU\n",
    "device = device(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downloading and Organizing the Dataset\n",
    "\n",
    "We'll use the provided `download_dataset.py` logic to download and organize the Pokémon dataset. Ensure you have your Kaggle API credentials set up in the .env file before running the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# Check if the data has already been downloaded and if not, download it\n",
    "if not os.path.exists(\"data/raw\"):\n",
    "    download_dataset(\n",
    "        dataset_name=\"bhawks/pokemon-generation-one-22k\",\n",
    "        raw_path=\"data/raw\",\n",
    "        processed_path=\"data/processed\",\n",
    "        extra_path=\"PokemonData\",\n",
    "    )\n",
    "    download_dataset(\n",
    "        dataset_name=\"thedagger/pokemon-generation-one\",\n",
    "        raw_path=\"data/raw\",\n",
    "        processed_path=\"data/processed\",\n",
    "        extra_path=\"dataset\",\n",
    "        skip_folders=[\n",
    "            \"Nidorina\",\n",
    "            \"Nidorino\",\n",
    "        ],  # Skip these folders since nidoran-f and nidoran-m are accidentally in these folders\n",
    "    )\n",
    "    download_dataset(\n",
    "        dataset_name=\"mikoajkolman/pokemon-images-first-generation17000-files\",\n",
    "        raw_path=\"data/raw\",\n",
    "        processed_path=\"data/processed\",\n",
    "        extra_path=\"pokemon\",\n",
    "        skip_folders=[\n",
    "            \"Nidorina\",\n",
    "            \"Nidorino\",\n",
    "        ],  # Skip these folders since nidoran-f and nidoran-m are accidentally in these folders\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading, preprocessing, and setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'kantodex-classifier (Python 3.11.10)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n kantodex-classifier ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Load the configuration file\n",
    "config = load_config(\"src/config/config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dummy data to get number of classes\n",
    "_, val_loader_dummy, label_to_idx_dummy = load_data(\n",
    "    processed_path=config[\"data\"][\"processed_path\"],\n",
    "    test_size=config[\"data\"][\"test_size\"],\n",
    "    batch_size=1,\n",
    "    img_size=tuple(config[\"data\"][\"img_size\"]),\n",
    "    num_workers=config[\"data\"][\"num_workers\"],\n",
    ")\n",
    "num_classes = len(label_to_idx_dummy)\n",
    "logging.info(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Initialize model\n",
    "model = initialize_model(config, num_classes, device)\n",
    "\n",
    "# Enable tensorboard logging\n",
    "\n",
    "\n",
    "tensorboard_logger = TensorBoardLogger(\n",
    "    log_dir=\"runs/\",\n",
    "    enabled=True,\n",
    ")\n",
    "\n",
    "sample_images, _ = next(iter(val_loader_dummy))\n",
    "sample_images = sample_images.to(device)\n",
    "tensorboard_logger.add_graph(model, sample_images)\n",
    "\n",
    "\n",
    "# Load actual train and val loaders\n",
    "train_loader, val_loader, label_to_idx = load_data(\n",
    "    processed_path=config[\"data\"][\"processed_path\"],\n",
    "    test_size=config[\"data\"][\"test_size\"],\n",
    "    batch_size=config[\"training\"].get(\"batch_size\", 32),\n",
    "    img_size=tuple(config[\"data\"][\"img_size\"]),\n",
    "    num_workers=config[\"data\"][\"num_workers\"],\n",
    ")\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = initialize_optimizer(config, model)\n",
    "scheduler = initialize_scheduler(config, optimizer)\n",
    "\n",
    "# Initialize loss function\n",
    " # Loss with Label Smoothing or FocalLoss with class weights\n",
    "label_smoothing = config[\"training\"].get(\"label_smoothing\", 0.0)\n",
    "if label_smoothing > 0.0:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    logging.info(f\"Using CrossEntropyLoss with label smoothing: {label_smoothing}\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logging.info(\"Using CrossEntropyLoss\")\n",
    "\n",
    "\n",
    "# Metrics\n",
    "metrics_calculator = MetricsCalculator(num_classes=num_classes)\n",
    "# Mixed Precision Scaler\n",
    "scaler = GradScaler(enabled=device.type == \"cuda\")\n",
    "# Early Stopping parameters\n",
    "early_stopping_patience = config[\"training\"].get(\"early_stopping_patience\", 10)\n",
    "epochs_no_improve = 0\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Mixup initialization if needed\n",
    "use_mixup = config[\"augmentation\"].get(\"use_mixup\", False)\n",
    "use_cutmix = config[\"augmentation\"].get(\"use_cutmix\", False)\n",
    "mixup_fn = None\n",
    "if use_mixup or use_cutmix:\n",
    "    # timm Mixup handles both based on provided arguments\n",
    "    mixup_fn = Mixup(\n",
    "        mixup_alpha=config[\"augmentation\"].get(\"alpha\", 1.0) if use_mixup else 0.0,\n",
    "        cutmix_alpha=config[\"augmentation\"].get(\"alpha\", 1.0) if use_cutmix else 0.0,\n",
    "        label_smoothing=label_smoothing,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    logging.info(\"Using Mixup/CutMix from timm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(  # noqa: PLR0913\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: device,\n",
    "    metrics_calculator: MetricsCalculator,\n",
    "    tensorboard_logger: TensorBoardLogger | None = None,\n",
    "    epoch: int = 0,\n",
    "    idx_to_label: dict[int, str] | None = None,\n",
    ") ->tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Validate the model on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to validate.\n",
    "        dataloader (DataLoader): DataLoader for validation data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        device (torch.device): Device to perform validation on (CPU or CUDA).\n",
    "        metrics_calculator (MetricsCalculator): Instance to calculate validation metrics.\n",
    "        tensorboard_logger (Optional[TensorBoardLogger], optional): Logger for TensorBoard.\n",
    "            Default is `None`.\n",
    "        epoch (int, optional): Current epoch number for logging purposes. Default is `0`.\n",
    "        idx_to_label (Optional[dict[int, str]], optional): Dictionary mapping label indices to\n",
    "            class names. Default is `None`.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: Tuple containing the average validation loss and the validation\n",
    "            accuracy.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metrics_calculator.reset()\n",
    "\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\", unit=\"batch\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            metrics_calculator.update(outputs, labels)\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(dataloader)\n",
    "    metrics = metrics_calculator.compute()\n",
    "\n",
    "    if tensorboard_logger:\n",
    "        epoch_val_accuracy = metrics.get(\"accuracy\", 0.0)\n",
    "        precision = metrics.get(\"precision\", 0.0)\n",
    "        recall = metrics.get(\"recall\", 0.0)\n",
    "        f1_score = metrics.get(\"f1\", 0.0)\n",
    "        auroc_score = metrics.get(\"auroc\", 0.0)\n",
    "\n",
    "        tensorboard_logger.add_scalar(\"Epoch Validation Accuracy\", epoch_val_accuracy, epoch + 1)\n",
    "        tensorboard_logger.add_scalar(\"Epoch Validation Loss\", avg_val_loss, epoch + 1)\n",
    "        tensorboard_logger.add_scalar(\"Epoch Validation Precision\", precision, epoch + 1)\n",
    "        tensorboard_logger.add_scalar(\"Epoch Validation Recall\", recall, epoch + 1)\n",
    "        tensorboard_logger.add_scalar(\"Epoch Validation F1 Score\", f1_score, epoch + 1)\n",
    "        tensorboard_logger.add_scalar(\"Epoch Validation Auroc score\", auroc_score, epoch + 1)\n",
    "\n",
    "        if idx_to_label:\n",
    "            tensorboard_logger.add_class_accuracy(\n",
    "                class_names=idx_to_label,\n",
    "                class_accuracy=metrics.get(\"per_class_accuracy\", {}),\n",
    "                global_step=epoch,\n",
    "            )\n",
    "            tensorboard_logger.add_confusion_matrix(\n",
    "                confusion_matrix=metrics.get(\"confusion_matrix\", np.array([])),\n",
    "                class_names=idx_to_label,\n",
    "                global_step=epoch,\n",
    "            )\n",
    "            worst_performing = metrics.get(\"worst_performing_classes\", [])[:15]\n",
    "            logging.info(f\"Worst performing classes: {worst_performing}\")\n",
    "            tensorboard_logger.add_text(\n",
    "                \"Worst Performing Classes\",\n",
    "                str(worst_performing),\n",
    "                epoch,\n",
    "            )\n",
    "            for idx, acc in metrics.get(\"per_class_accuracy\", {}).items():\n",
    "                class_name = idx_to_label.get(idx, f\"Class_{idx}\")\n",
    "                tensorboard_logger.add_scalar(\n",
    "                    f\"Class Accuracy/{class_name}\",\n",
    "                    acc,\n",
    "                    epoch,\n",
    "                )\n",
    "        logging.info(\n",
    "            f\"Epoch Validation Loss: {avg_val_loss:.4f}, Precision: {precision:.2f}%, \"\n",
    "            f\"Recall: {recall:.2f}%, F1 Score: {f1_score:.2f}%, AUROC: {auroc_score:.2f}%\",\n",
    "        )\n",
    "\n",
    "    return avg_val_loss, metrics.get(\"accuracy\", 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "learning_rates = []\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = config[\"training\"].get(\"epochs\", 100)\n",
    "for epoch in range(0, num_epochs):\n",
    "    logging.info(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    train_loss, train_accuracy = train_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        scaler=scaler,\n",
    "        metrics_calculator=metrics_calculator,\n",
    "        tensorboard_logger=tensorboard_logger,\n",
    "        mixup_fn=mixup_fn,\n",
    "        epoch=epoch + 1,\n",
    "    )\n",
    "    logging.info(\n",
    "        f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "        f\"Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\",\n",
    "    )\n",
    "\n",
    "    metrics = metrics_calculator.compute()\n",
    "    precision = metrics.get(\"precision\", 0.0)\n",
    "    recall = metrics.get(\"recall\", 0.0)\n",
    "    f1_score = metrics.get(\"f1\", 0.0)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "    logging.info(\n",
    "        f\"Precision: {precision:.2f}%, Recall: {recall:.2f}%, F1 Score: {f1_score:.2f}%\",\n",
    "    )\n",
    "\n",
    "\n",
    "    if tensorboard_logger:\n",
    "        tensorboard_logger.add_scalar(\"Training Precision\", precision, epoch + 1)\n",
    "        tensorboard_logger.add_scalar(\"Training Recall\", recall, epoch + 1)\n",
    "        tensorboard_logger.add_scalar(\"Training F1 Score\", f1_score, epoch + 1)\n",
    "\n",
    "    # Scheduler step\n",
    "    if scheduler:\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(train_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        logging.info(f\"Learning Rate: {current_lr}\")\n",
    "        if tensorboard_logger:\n",
    "            tensorboard_logger.add_scalar(\"Training Learning Rate\", current_lr, epoch + 1)\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "    val_loss, val_accuracy = validate(\n",
    "        model=model,\n",
    "        dataloader=val_loader,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        metrics_calculator=metrics_calculator,\n",
    "        tensorboard_logger=tensorboard_logger,\n",
    "        epoch=epoch + 1,\n",
    "        idx_to_label=idx_to_label,\n",
    "    )\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    logging.info(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # Check for best model\n",
    "    is_best = val_accuracy > best_accuracy\n",
    "    if is_best:\n",
    "        best_accuracy = val_accuracy\n",
    "        epochs_no_improve = 0\n",
    "        logging.info(f\"New best accuracy: {best_accuracy:.2f}%\")\n",
    "        if tensorboard_logger:\n",
    "            tensorboard_logger.add_scalar(\"Best Accuracy\", best_accuracy, epoch + 1)\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        logging.info(f\"No improvement in validation accuracy for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_dir = Path(config[\"training\"].get(\"checkpoint_path\", \"checkpoints\"))\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    save_checkpoint(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch + 1,\n",
    "        checkpoint_dir=str(checkpoint_dir),\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        is_best=is_best,\n",
    "        best_accuracy=best_accuracy,\n",
    "    )\n",
    "\n",
    "    if epochs_no_improve >= early_stopping_patience:\n",
    "        logging.info(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "logging.info(\"Training complete.\")\n",
    "if tensorboard_logger:\n",
    "    tensorboard_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Training Results\n",
    "\n",
    "Let's plot the training and validation loss and accuracy over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss and Accuracy\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Confusion Matrix\n",
    "\n",
    "After training, we'll evaluate the model on the validation set and create a confusion matrix to understand where the model is making mistakes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(checkpoint_dir / \"best_model.pth\")[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Collect all predictions and labels\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_norm = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "\n",
    "plt.figure(figsize=(20, 18))\n",
    "sns.heatmap(cm_norm, annot=False, cmap='Blues')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(all_labels, all_preds, target_names=[idx_to_label[i] for i in range(num_classes)])\n",
    "print(report)\n",
    "\n",
    "# Identify most confused classes\n",
    "def get_most_confused(cm: np.ndarray, idx_to_label: dict[int, str], top_n: int = 5) -> list[tuple[str, str, float]]:\n",
    "    cm_copy = cm.copy().astype(float)\n",
    "    np.fill_diagonal(cm_copy, 0)  # Remove correct predictions\n",
    "    confusions = []\n",
    "    for i in range(cm_copy.shape[0]):\n",
    "        for j in range(cm_copy.shape[1]):\n",
    "            if cm_copy[i, j] > 0:\n",
    "                confusions.append((idx_to_label[i], idx_to_label[j], cm_copy[i, j]))\n",
    "    # Sort by highest confusion\n",
    "    confusions_sorted = sorted(confusions, key=lambda x: x[2], reverse=True)\n",
    "    return confusions_sorted[:top_n]\n",
    "\n",
    "most_confused = get_most_confused(cm, idx_to_label, top_n=5)\n",
    "print(\"Most Confused Classes:\")\n",
    "for true_label, pred_label, count in most_confused:\n",
    "    print(f\"{true_label} confused with {pred_label}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference on Sample Images\n",
    "\n",
    "We'll run inference on a subset of validation images and display them with the predicted labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images with predictions\n",
    "def display_predictions(model: nn.Module, dataloader: DataLoader, idx_to_label: dict[int, str], num_images: int = 5):\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            with torch.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "            for i in range(images.size(0)):\n",
    "                if images_shown >= num_images:\n",
    "                    break\n",
    "                img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "                img = np.clip((img * np.array([0.229, 0.224, 0.225])) +\n",
    "                              np.array([0.485, 0.456, 0.406]), 0, 1)\n",
    "                plt.subplot(1, num_images, images_shown + 1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Pred: {idx_to_label[predicted[i].item()]}\\nTrue: {idx_to_label[labels[i].item()]}\")\n",
    "                plt.axis('off')\n",
    "                images_shown += 1\n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "    plt.show()\n",
    "\n",
    "# Display predictions on 5 validation images\n",
    "display_predictions(model, val_loader, idx_to_label, num_images=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kantodex-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
